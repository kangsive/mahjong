# 现代麻将高级算法研究报告

## 引言

麻将，作为一款拥有悠久历史的策略游戏，其复杂性不仅体现在规则的多样性上，更在于其融合了概率论、博弈论和心理学等多个维度的深度策略。每一局麻将都是一个在不完美信息（无法看到对手手牌）和随机性（摸牌）下的多智能体博弈过程。

传统的麻将技巧多依赖于人类玩家的经验、直觉和简单的统计归纳。然而，随着计算机科学和人工智能（AI）的飞速发展，现代麻将的策略研究已经进入了一个全新的数据驱动和计算建模时代。一方面，系统化的理论如日本立直麻将的"牌效率"和"防守理论"被提出，为人类玩家提供了清晰的指导框架。另一方面，以微软亚洲研究院的`Suphx`和腾讯的`绝艺LuckyJ`为代表的麻将AI，通过深度学习和强化学习等尖端技术，达到了超越顶尖职业选手的水平，它们不仅验证了现有理论，甚至发现了人类难以企及的全新策略维度。

本报告旨在深度研究现代麻将中的高级算法与理论，并从两个维度进行剖析：
1.  **哪些算法和方法适合设计成强大的AI玩家？**
2.  **哪些理论和方法适合人类玩家学习并提升自身水平？**

通过本次研究，我们将清晰地描绘出人类智慧与机器智能在麻将这一复杂博弈领域中的不同进路与各自优势。

---

## Part 1: 面向人类玩家的策略理论

这类理论的特点是具有高度的"可解释性"和"可执行性"，它们被设计为人类可以在有限的思考时间内理解并应用的指导原则。这些理论通常是顶尖玩家经验的高度概括和数据化总结。

### 1.1 向听数 (Shanten) 与牌效率 (Tile Efficiency)

这是现代麻将攻防决策的基石，核心目标是让手牌尽快达到听牌（Tenpai）状态。

-   **核心思想**:
    向听数（Shanten Number）是衡量手牌距离听牌状态最少还需要替换多少张牌的数值。0向听即为听牌，1向听即差一张牌听牌，以此类推。牌效率（Tile Efficiency）的核心就是，在每一步决策时，选择一种打法（通常是打出某张牌），使得手牌的向听数最小化，或者在向听数不变的情况下，最大化下一步能够减少向ঠি数的有效进张（日语称为"受け入れ"，Ukeire）。

-   **具体方法和步骤**:
    1.  **计算向听数**: 标准麻将牌型（4副面子+1对将）的向听数通常使用公式 `8 - (面子数 * 2 + 搭子数 + 对子数)` 来估算。一个更精确的方法是计算 "面子" 和 "搭子" 的数量。
        *   **面子 (Mentsu)**: 已经完成的顺子（如123万）或刻子（如三个东风）。
        *   **搭子 (Taatsu)**: 还差一张牌就能形成面子的组合，如两面搭子（23万）、嵌张搭子（46条）、边张搭子（89筒）或对子（两个发财）。
        *   目标是凑齐 **4个面子和1个对子（雀头）**。向听数就是距离这个目标还差几步。
    2.  **评估打哪张牌**: 面对14张手牌，轮到自己打牌时，依次假设打出每一张牌，然后计算打出后剩下13张牌的"有效进张总数"。
    3.  **最大化有效进张**: 选择那个能使有效进张种类和数量最大化的打法。例如，在"23456万"中，打出"2万"剩下"3456万"，听2、5、8万；而打出"6万"剩下"2345万"，听1、4、7万。如果牌池中8万已经出现很多，则打6万是更优的选择。

-   **学习适用性**:
    这个理论是**最适合人类玩家学习的核心技能**。它将复杂的牌面判断简化为一个可量化的优化问题。玩家通过练习，可以快速判断手牌结构，估算不同打法的优劣，从而做出数据上最优的进攻选择。网络上已有大量的牌效训练工具帮助玩家熟悉这一理论。

### 1.2 防守与危险牌评估 (Risk Assessment)

当判断自己和牌希望不大，或有其他玩家明显听牌（例如立直）时，就需要转向防守，尽量避免打出"铳牌"（点炮牌）。

-   **核心思想**:
    通过场上已经公开的信息（各家的舍牌河、副露、宝牌指示牌），推断其他玩家可能需要的牌，从而评估自己手中每一张牌的"危险度"。

-   **具体方法和步骤**:
    1.  **筋牌理论 (Suji)**: 这是最基础的危险度判断理论。如果一个玩家打出了"4万"，那么他听"23万"或"56万"这两组牌的概率就会降低（因为他很可能已经拆掉了对应的搭子）。因此，对于这个玩家来说，"1万"和"7万"（4万的筋牌）的危险度就相对较低。常见的筋牌组合有1-4-7, 2-5-8, 3-6-9。
    2.  **壁理论 (Kabe)**: 当某张数牌在场上已经全部可见（4张都在牌河、副露或自己手里），这张牌就成为了"壁"。例如，所有"7筒"都可见，那么任何玩家都不可能听需要"7筒"才能组成的顺子，如"56筒"听"47筒"或"89筒"听"7筒"。因此，"壁"外侧的牌（如8筒、9筒）的危险度会显著降低。
    3.  **早巡舍牌 (Early Discards)**: 游戏初期（前5巡）打出的牌，因为当时手牌尚未成型，所以这些牌周围的牌通常比较安全。
    4.  **生张与现物 (Sho-hai & Genbutsu)**: "现物"是指某个玩家舍牌河里已经有的牌，是对他100%安全牌。反之，从未在场上出现过的字牌或中张（3-7的数牌）被称为"生张"，危险度通常很高。

-   **学习适用性**:
    防守理论同样**非常适合人类玩家学习**。它提供了一套行之有效的逻辑框架来推断看不见的信息。虽然不能100%避免点炮，但能极大提高防守成功率。高级玩家能综合运用多种理论，并结合玩家的打牌风格，做出极其精准的危险度判断。

---

## Part 2: 面向 AI 玩家的计算模型

AI玩家的"思考"方式与人类截然不同。AI不依赖直觉，而是通过强大的算力和先进的算法，从海量数据中寻找最优解。

### 2.1 蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)

MCTS是一种在很多棋类游戏中取得巨大成功的搜索算法，如AlphaGo。它能有效地在巨大的游戏状态空间中进行启发式搜索。

-   **核心思想**:
    当AI需要决策时，它从当前的游戏状态（根节点）出发，通过模拟大量的随机对局来评估每个可能动作的优劣。一个动作如果能在多次模拟中带来较高的胜率，就会被认为是好动作。

-   **具体方法和步骤**:
    1.  **选择 (Selection)**: 从根节点开始，根据一种策略（如UCT算法，平衡探索与利用）选择一个最优的子节点，直到达到一个未完全探索的节点。
    2.  **扩展 (Expansion)**: 在这个节点上，创建一个或多个新的子节点，代表新的可能动作。
    3.  **模拟 (Simulation/Rollout)**: 从新的子节点开始，用一个快速的、通常是随机的策略进行一次完整的模拟对局，直到游戏结束。
    4.  **反向传播 (Backpropagation)**: 将模拟对局的结果（赢或输）沿着路径传回，更新从根节点到该子节点路径上所有节点的统计数据（如模拟次数和获胜次数）。
    5.  重复以上步骤成千上万次后，AI选择根节点下统计数据最优的那个动作。

-   **AI适用性**:
    MCTS**非常适合构建麻将AI**。对于不完美信息，AI可以在模拟前先"猜测"或"采样"一个对手可能的手牌（称为Determinization），然后在这个"确定"的局面下进行模拟。通过海量模拟，MTCS能够超越简单的牌效率计算，考虑到更长远的攻防收益。

### 2.2 监督学习 (Supervised Learning)

这是构建AI的经典方法，通过"模仿"人类专家来学习。

-   **核心思想**:
    让AI模型（通常是深度神经网络，如CNN）学习一个庞大的、由人类高手对局组成的数据集。模型的目标是预测在给定局面下，人类高手会打出哪张牌，或者是否会吃、碰、杠。

-   **具体方法和步骤**:
    1.  **数据收集**: 收集数百万局在线麻将平台（如"天凤"）的高段位玩家对局记录。
    2.  **特征工程**: 将麻将的局面（手牌、牌河、副露、宝牌等）编码成一个多维的张量（类似于一张图片），作为神经网络的输入。
    3.  **模型训练**: 训练一个神经网络，使其输出的动作（如打出34种牌的概率分布）与数据集中人类高手的实际动作尽可能一致。
    4.  **决策**: 在实际游戏中，AI将当前局面输入训练好的模型，直接选择输出概率最高的动作。

-   **AI适用性**:
    监督学习是训练麻将AI的**有效初始步骤**。它可以让AI快速达到接近人类高手的水平。`Suphx`的第一个版本就是主要基于监督学习。但它的缺点是，AI的水平上限受限于数据集中人类的水平，无法超越人类。

### 2.3 深度强化学习 (Deep Reinforcement Learning, DRL)

这是当前最前沿的AI训练方法，核心思想是让AI通过"自我博弈"来学习和进化，从而超越人类。

-   **核心思想**:
    AI不再模仿人类，而是在一个虚拟环境中与自己（或自己的不同历史版本）进行数亿甚至数十亿局的对战。通过"试错"来学习。系统会根据每局的最终得分（奖励或惩罚）来调整其决策网络，目标是学习一套能最大化长期累计奖励的策略。

-   **具体方法和步骤**:
    1.  **自我对战 (Self-Play)**: 两个AI（或多个）在模拟环境中对战。
    2.  **策略评估与更新**: 使用Actor-Critic、PPO等算法。**Actor**（策略网络）负责根据当前局面做出决策，**Critic**（价值网络）负责评估当前局面的好坏或Actor决策的优劣。
    3.  **奖励设计**: 奖励函数的设计至关重要。最简单的奖励是最终得分，但更复杂的模型（如`Suphx`）会设计更精细的奖励函数来评估中间状态。
    4.  **不断迭代**: AI通过海量的自我对战，不断微调其神经网络参数。这个过程会发现很多人类未曾想到的策略，因为它唯一的目标就是"赢"，不受人类固有思维的束缚。
    5.  **课程学习 (Curriculum Learning)**: 如北大论文中所述，为了提高训练效率和稳定性，可以让AI从简单的局面（如接近听牌的牌局）开始学习，然后逐渐增加难度，最后过渡到完全随机的初始牌局。

-   **AI适用性**:
    DRL是目前构建**顶级麻将AI的不二之选**。它能够达到远超人类的水平。`Suphx`和`绝艺LuckyJ`的成功都证明了这一点。这种方法计算量巨大，需要强大的硬件支持，但其达到的高度也是前所未有的。

---

## Part 3: 算法适用性分析

### 3.1 适合设计AI玩家使用的方法

**深度强化学习（DRL）结合蒙特卡洛树搜索（MCTS）** 是当前设计顶尖AI玩家最适合的范式。

-   **为什么适合AI**:
    1.  **超越人类上限**: DRL通过自我博弈，能够探索整个人类知识之外的策略空间，发现最优解。
    2.  **处理高维状态**: 深度神经网络能够从复杂的麻将局面中自动提取有效特征，无需人工设计复杂的规则。
    3.  **长期规划能力**: 强化学习的目标是最大化长期收益，这使得AI能够做出具有大局观的决策，例如为了做成价值更高的牌型而暂时牺牲牌效率，或者在早期就进行精准的防守布局。
    4.  **算力优势**: AI可以利用强大的计算资源进行24/7不间断的训练和模拟，这是人类无法比拟的。MCTS的海量模拟和DRL的海量对局都是建立在这一基础之上。

监督学习可以作为AI的"启蒙老师"，为其提供一个高水平的起点，但最终要达到巅峰，必须依赖于强化学习的自我进化。

### 3.2 适合人类玩家学习使用的方法

**牌效率理论和危险度评估** 是最适合人类玩家学习的理论。

-   **为什么适合人类**:
    1.  **认知友好**: 这些理论被总结为一套清晰、可理解的规则和启发式方法（如筋牌、壁），符合人类的认知和记忆习惯。
    2.  **实时决策**: 人类玩家需要在十几秒内做出决策，不可能像AI一样进行海量模拟。这些理论提供了一套"心算"工具，可以在有限时间内做出高质量的决策。
    3.  **构建基础**: 掌握这些基础理论是成为麻将高手的前提。它们构成了人类玩家策略大厦的基石，在此之上才能发展出更高级的读牌、牌桌控制和心理博弈技巧。
    4.  **可借助工具练习**: 存在大量在线的牌效练习器、何切分析工具，可以帮助玩家系统地训练和掌握这些理论。

AI的算法（如MCTS的完整流程或DRL的神经网络权重）对人类来说是"黑箱"的，无法直接学习。但人类可以**学习AI的决策结果**，通过复盘AI的牌局，分析AI在关键节点的选择，可以反过来启发人类，发现新的策略，并可能最终将这些新策略总结成新的、可供人类理解的理论。

---

## 结论

现代麻将的高级算法和理论沿着两条截然不同的路径发展：

-   **人类的路径**是**"知识化"和"规则化"**。通过总结和归纳，将复杂的博弈经验提炼为可学习、可传承的理论体系，如牌效率和防守理论。这条路径的优势在于高效、易于理解和实践，是人类智慧的结晶。

-   **AI的路径**是**"计算化"和"最优化"**。通过穷举式的模拟和无休止的自我博弈，在庞大的策略空间中直接搜索最优解。这条路径的优势在于其无与伦比的精度和超越人类直觉的深度，是机器智能的体现。

因此，对于想要提升自己水平的麻将玩家来说，最有效的方法是系统学习牌效率和防守理论。而对于研究者和开发者来说，深度强化学习则是打造最强麻将AI的核心技术。未来，这两条路径将继续交织，AI的惊人决策将不断为人类麻将理论提供新的素材和灵感。

---

## References & Sources

1.  **硕士研究生学位论文: 一种用于国标麻将 AI 强化学习训练的课程学习方法**. (提供了关于课程学习、强化学习在麻将中应用的优秀综述)
    -   Source: `https://ai.pku.edu.cn/docs/2024-07/20240704001356567345.pdf`

2.  **Suphx: Mastering Mahjong with Deep Reinforcement Learning**. (微软亚洲研究院关于其麻将AI `Suphx` 的论文)
    -   Source: `https://arxiv.org/abs/2003.13590`

3.  **Method for Constructing Artificial Intelligence Player with Abstractions to Markov Decision Processes in Multiplayer Game of Mahjong**. (关于使用马尔可夫决策过程抽象麻将游戏的研究)
    -   Source: `https://arxiv.org/pdf/1904.07491.pdf`

4.  **Analysis of the Application of Artificial Intelligence in Mahjong**. (一篇关于麻将中AI应用分析的综述性文章)
    -   Source: `https://www.deanfrancispress.com/index.php/te/article/view/1769` 